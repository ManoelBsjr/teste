{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def cria_contexto_spark():\n",
    "    global APP_NAME\n",
    "    sparkSession = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "    sc = SparkContext.getOrCreate() \n",
    "    return sc\n",
    "def cria_contexto_sql(spark_context):\n",
    "    sqlContext = SQLContext(spark_context) \n",
    "    return sqlContext\n",
    "\n",
    "APP_NAME = \"Teste_rox\"\n",
    "sc = cria_contexto_spark()\n",
    "spark = cria_contexto_sql(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "df_person_person = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Person.Person.csv\", sep=';', header=\"true\")\n",
    "df_production_product = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Production.Product.csv\", sep=';', header=\"true\")\n",
    "df_sales_customer = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Sales.Customer.csv\", sep=';', header=\"true\")\n",
    "df_sales_detail = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Sales.SalesOrderDetail.csv\", sep=';', header=\"true\")\n",
    "df_sales_header = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Sales.SalesOrderHeader.csv\", sep=';', header=\"true\")\n",
    "df_sales_special_offer = spark.read.csv(\"C:\\\\Users\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\TesteRox\\\\Engenheiro de Dados - CSV\\\\Sales.SpecialOfferProduct.csv\", sep=';', header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ítem 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Após um printSchema, percebe-se que todas as colunas estão definidas como String\n",
    "#Converter para Integer a coluna \"SalesOrderDetailID\" para tornar possivel operaçoes matemáticas\n",
    "df_sales_detail = df_sales_detail.withColumn(\"SalesOrderDetailID\", df_sales_detail.SalesOrderDetailID.cast('integer'))\n",
    "\n",
    "#Registrar como Tabela para executar comandos SQL\n",
    "spark.registerDataFrameAsTable(df_sales_detail, \"df_sales_detail\")\n",
    "\n",
    "#Consulta em SQL a quantidade de números distintos de SalesOrderID onde a coluna SalesOrderDetailID seja maior ou igual a 3.\n",
    "df_qtd_salesorderid_distincts = spark.sql(\"\"\"SELECT\n",
    "                count(distinct SalesOrderID) AS Qtd_SalesOrderID_Distintos\n",
    "            FROM df_sales_detail\n",
    "            WHERE SalesOrderDetailID >= 3\n",
    "            \"\"\")\n",
    "#Path para salvar\n",
    "#df_qtd_salesorderid_distincts.write.format(\"csv\").option(\"header\", \"true\").save(\"C:\\\\Users\\\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\\\EngDados\\\\roxpartner\\\\tb_item1.csv\",sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|Qtd_SalesOrderID_Distintos|\n",
      "+--------------------------+\n",
      "|                     31465|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_qtd_salesorderid_distincts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ítem 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 1:\n",
    "\n",
    "##Registrar DF´s como tabela para executar Queries em SQL\n",
    "spark.registerDataFrameAsTable(df_sales_special_offer, \"df_sales_special_offer\")\n",
    "spark.registerDataFrameAsTable(df_production_product, \"df_production_product\")\n",
    "\n",
    "#Primeiro passo Pegar os ProductID em ofertas e acrescentar a coluna \"Name\" e \"DaysToManufacture\" \n",
    "#através de um Left Join com a tabela df_production_product com estratégia => a.ProductID = b.ProductID\n",
    "\n",
    "df_sales_offer_names = spark.sql(\"\"\"SELECT \n",
    "        b.Name, \n",
    "        b.DaysToManufacture,\n",
    "        a.ProductID\n",
    "    FROM df_sales_special_offer AS a\n",
    "    LEFT JOIN df_production_product AS b\n",
    "    ON a.ProductID = b.ProductID\n",
    "    \"\"\")\n",
    "\n",
    "###Registrar novamente o DF como tabela para executar Queries em SQL\n",
    "spark.registerDataFrameAsTable(df_sales_offer_names, \"df_sales_offer_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+---------+\n",
      "|                Name|DaysToManufacture|ProductID|\n",
      "+--------------------+-----------------+---------+\n",
      "|HL Road Frame - B...|                1|      680|\n",
      "|HL Road Frame - R...|                1|      706|\n",
      "|Sport-100 Helmet,...|                0|      707|\n",
      "|Sport-100 Helmet,...|                0|      708|\n",
      "|Mountain Bike Soc...|                0|      709|\n",
      "|Mountain Bike Soc...|                0|      710|\n",
      "|Sport-100 Helmet,...|                0|      711|\n",
      "|        AWC Logo Cap|                0|      712|\n",
      "|Long-Sleeve Logo ...|                0|      713|\n",
      "|Long-Sleeve Logo ...|                0|      714|\n",
      "|Long-Sleeve Logo ...|                0|      715|\n",
      "|Long-Sleeve Logo ...|                0|      716|\n",
      "|HL Road Frame - R...|                1|      717|\n",
      "|HL Road Frame - R...|                1|      718|\n",
      "|HL Road Frame - R...|                1|      719|\n",
      "|HL Road Frame - R...|                1|      720|\n",
      "|HL Road Frame - R...|                1|      721|\n",
      "|LL Road Frame - B...|                1|      722|\n",
      "|LL Road Frame - B...|                1|      723|\n",
      "|LL Road Frame - B...|                1|      724|\n",
      "+--------------------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_offer_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 2:\n",
    "\n",
    "#converter a coluna OrderQty para int para posteriormente usar operaçao para somar \n",
    "df_sales_detail = df_sales_detail.withColumn(\"OrderQty\", df_sales_detail.OrderQty.cast('integer'))\n",
    "\n",
    "#Registrar como tabela para executar Queries em SQL\n",
    "spark.registerDataFrameAsTable(df_sales_detail, \"df_sales_detail\")\n",
    "\n",
    "# top 3 de ProductID com maiores números de OrderQty\n",
    "df_top3_orderQty = spark.sql(\"\"\"SELECT \n",
    "        ProductID,\n",
    "        SUM(OrderQty) as Total_OrderQty\n",
    "    FROM df_sales_detail\n",
    "    GROUP BY ProductID\n",
    "    ORDER BY Total_OrderQty DESC\n",
    "    LIMIT 3\"\"\")\n",
    "spark.registerDataFrameAsTable(df_top3_orderQty, \"df_top3_orderQty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|ProductID|Total_OrderQty|\n",
      "+---------+--------------+\n",
      "|      712|          8311|\n",
      "|      870|          6815|\n",
      "|      711|          6743|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top3_orderQty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo3: Selectionar as colunas \"Name\" e \"DaysToManufacture\" da tabela df_sales_offer_names \n",
    "#Juntar com a coluna \"Total_OrderQty\" da tabela df_top3_orderQty por meio de um Left Join onde \"a.ProductID = b.ProductID\"\n",
    "#Para retornar os 3 produtos com maior \"Total_OrderQty\"\n",
    "\n",
    "\n",
    "df_products_top3 = spark.sql(\"\"\"SELECT \n",
    "        b.Name,\n",
    "        b.DaysToManufacture,\n",
    "        a.Total_OrderQty\n",
    "    FROM df_top3_orderQty AS a\n",
    "    LEFT JOIN df_sales_offer_names AS b\n",
    "    ON a.ProductID = b.ProductID\n",
    "    GROUP BY Name, DaysToManufacture, Total_OrderQty\n",
    "    ORDER BY Total_OrderQty DESC\n",
    "    \"\"\")\n",
    "\n",
    "#df_products_top3.write.format(\"csv\").option(\"header\", \"true\").save(\"C:\\\\Users\\\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\\\EngDados\\\\roxpartner\\\\tb_item2.csv\",sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+--------------+\n",
      "|Name                  |DaysToManufacture|Total_OrderQty|\n",
      "+----------------------+-----------------+--------------+\n",
      "|AWC Logo Cap          |0                |8311          |\n",
      "|Water Bottle - 30 oz. |0                |6815          |\n",
      "|Sport-100 Helmet, Blue|0                |6743          |\n",
      "+----------------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products_top3.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ítem 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 1:\n",
    "\n",
    "##Registrar DF´s como tabela para executar Queries em SQL\n",
    "spark.registerDataFrameAsTable(df_person_person, \"df_person_person\")\n",
    "spark.registerDataFrameAsTable(df_sales_customer, \"df_sales_customer\")\n",
    "spark.registerDataFrameAsTable(df_sales_header, \"df_sales_header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 2:\n",
    "df_name_id = spark.sql(\"\"\"SELECT \n",
    "        a.FirstName,\n",
    "        a.MiddleName,\n",
    "        a.LastName,\n",
    "        b.CustomerID\n",
    "    FROM df_sales_customer AS b\n",
    "    LEFT JOIN df_person_person AS a\n",
    "    ON a.BusinessEntityID = b.CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "spark.registerDataFrameAsTable(df_name_id, \"df_name_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 3:\n",
    "df_sell = spark.sql(\"\"\"SELECT \n",
    "        a.FirstName,\n",
    "        a.MiddleName,\n",
    "        a.LastName,\n",
    "        b.SalesOrderID   \n",
    "    FROM df_sales_header AS b\n",
    "    LEFT JOIN df_name_id AS a\n",
    "    ON a.CustomerID = b.CustomerID\n",
    "    WHERE FirstName is not Null\n",
    "    \"\"\")\n",
    "\n",
    "#concatenar as colunas\"FirstName\",\"MiddleName\",\"LastName\" para gerar uma nova coluna \"ClientName\" \n",
    "# e dropo as colunas antigas.\n",
    "df1 = df_sell.select(\"*\", concat(col(\"FirstName\"),lit(\" \"),col(\"MiddleName\")).alias(\"New_col\"))\n",
    "df1 = df1.select(\"*\", concat(col(\"New_col\"),lit(\" \"),col(\"LastName\")).alias(\"ClientName\"))\n",
    "df1 = df1.drop(\"FirstName\",\"MiddleName\",\"LastName\",\"New_col\")\n",
    "df_sell_final = df1.withColumn('ClientName', regexp_replace('ClientName', 'NULL', '')) #Remove os MiddleName \"NULL\"\n",
    "spark.registerDataFrameAsTable(df_sell_final, \"df_sell_final\")\n",
    "\n",
    "df_sell_final = spark.sql(\"\"\"SELECT \n",
    "        ClientName,\n",
    "        COUNT(Distinct SalesOrderID) AS Qtd_SalesOrderID\n",
    "    FROM df_sell_final\n",
    "    GROUP BY ClientName\n",
    "    ORDER BY Qtd_SalesOrderID DESC\n",
    "    \"\"\")\n",
    "\n",
    "#df_sell_final.write.format(\"csv\").option(\"header\", \"true\").save(\"C:\\\\Users\\\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\\\EngDados\\\\roxpartner\\\\tb_item3.csv\",sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|      ClientName|Qtd_SalesOrderID|\n",
      "+----------------+----------------+\n",
      "| Morgan C Miller|              28|\n",
      "|Jennifer  Taylor|              28|\n",
      "|   Grace R Lewis|              27|\n",
      "| Marshall M Shen|              27|\n",
      "| Ruben B Vazquez|              27|\n",
      "|     Grace J Lee|              27|\n",
      "|   Morgan  Lewis|              27|\n",
      "|Isabella B Moore|              27|\n",
      "|Morgan P Jackson|              27|\n",
      "|Natalie M Martin|              27|\n",
      "+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sell_final.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ítem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_header = df_sales_header.withColumn(\"OrderDate\", df_sales_header[\"OrderDate\"].cast(\"date\"))\n",
    "df_sales_detail = df_sales_detail.withColumn(\"OrderQty\",df_sales_detail.OrderQty.cast(\"integer\"))\n",
    "df_sales_detail = df_sales_detail.withColumn(\"ProductID\",df_sales_detail[\"ProductID\"].cast(\"integer\"))\n",
    "\n",
    "#Registra como tabela para executar Queries em sql\n",
    "spark.registerDataFrameAsTable(df_sales_header,\"df_sales_header\")\n",
    "spark.registerDataFrameAsTable(df_sales_detail,\"df_sales_detail\")\n",
    "spark.registerDataFrameAsTable(df_production_product,\"df_production_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_sales_detail >>> SalesOrderID  OrderQty ProductID \n",
    "#df_sales_header >>> SalesOrderID  OrderDate\n",
    "#df_production_pr>>>                        ProductID\n",
    "df_total_produtos = spark.sql(\"\"\"SELECT\n",
    "        a.SalesOrderID,\n",
    "        a.OrderQty,\n",
    "        b.OrderDate,\n",
    "        a.ProductID\n",
    "    FROM df_sales_detail AS a \n",
    "    LEFT JOIN df_sales_header AS b\n",
    "    ON a.SalesOrderID = b.SalesOrderID\n",
    "    \"\"\")\n",
    "\n",
    "spark.registerDataFrameAsTable(df_total_produtos,\"df_total_produtos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+---------+\n",
      "|SalesOrderID|OrderQty| OrderDate|ProductID|\n",
      "+------------+--------+----------+---------+\n",
      "|       43659|       1|2011-05-31|      776|\n",
      "|       43659|       3|2011-05-31|      777|\n",
      "|       43659|       1|2011-05-31|      778|\n",
      "|       43659|       1|2011-05-31|      771|\n",
      "|       43659|       1|2011-05-31|      772|\n",
      "|       43659|       2|2011-05-31|      773|\n",
      "|       43659|       1|2011-05-31|      774|\n",
      "|       43659|       3|2011-05-31|      714|\n",
      "|       43659|       1|2011-05-31|      716|\n",
      "|       43659|       6|2011-05-31|      709|\n",
      "|       43659|       2|2011-05-31|      712|\n",
      "|       43659|       4|2011-05-31|      711|\n",
      "|       43660|       1|2011-05-31|      762|\n",
      "|       43660|       1|2011-05-31|      758|\n",
      "|       43661|       1|2011-05-31|      745|\n",
      "|       43661|       1|2011-05-31|      743|\n",
      "|       43661|       2|2011-05-31|      747|\n",
      "|       43661|       4|2011-05-31|      712|\n",
      "|       43661|       4|2011-05-31|      715|\n",
      "|       43661|       2|2011-05-31|      742|\n",
      "+------------+--------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total_produtos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_produtos_dia = spark.sql(\"\"\"SELECT\n",
    "        b.ProductID,\n",
    "        a.OrderDate, \n",
    "        SUM(a.OrderQty) AS total_OrderQty\n",
    "    FROM df_total_produtos AS a\n",
    "    LEFT JOIN df_production_product AS b\n",
    "    ON  a.ProductID = b.ProductID\n",
    "    GROUP BY b.ProductID, a.OrderDate  \n",
    "    ORDER BY a.OrderDate, ProductID\n",
    "     \n",
    "    \"\"\")\n",
    "#df_produtos_dia.write.format(\"csv\").option(\"header\", \"true\").save(\"C:\\\\Users\\\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\\\EngDados\\\\roxpartner\\\\tb_item4.csv\",sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|ProductID| OrderDate|total_OrderQty|\n",
      "+---------+----------+--------------+\n",
      "|      707|2011-05-31|            24|\n",
      "|      708|2011-05-31|            27|\n",
      "|      709|2011-05-31|            38|\n",
      "|      710|2011-05-31|             5|\n",
      "|      711|2011-05-31|            33|\n",
      "|      712|2011-05-31|            40|\n",
      "|      714|2011-05-31|            16|\n",
      "|      715|2011-05-31|            49|\n",
      "|      716|2011-05-31|            19|\n",
      "|      722|2011-05-31|             8|\n",
      "|      725|2011-05-31|            15|\n",
      "|      726|2011-05-31|             9|\n",
      "|      729|2011-05-31|            16|\n",
      "|      730|2011-05-31|            14|\n",
      "|      732|2011-05-31|            16|\n",
      "|      733|2011-05-31|             4|\n",
      "|      738|2011-05-31|            19|\n",
      "|      741|2011-05-31|             2|\n",
      "|      742|2011-05-31|             3|\n",
      "|      743|2011-05-31|             1|\n",
      "+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_produtos_dia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ítem 5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31465"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_header.count()   # SalesOrderID  OrderDate  TotalDue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_header = df_sales_header.withColumn(\"SalesOrderID\",df_sales_header[\"SalesOrderID\"].cast(\"integer\"))\n",
    "df_sales_header = df_sales_header.withColumn(\"OrderDate\",df_sales_header.OrderDate.cast(\"date\"))\n",
    "\n",
    "#Substituindo as \",\" da TotalDue por \".\"  , pra poder manipular em float\n",
    "df_sales_header = df_sales_header.withColumn('TotalDue', regexp_replace('TotalDue', ',', '.'))\n",
    "df_sales_header = df_sales_header.withColumn(\"TotalDue\",df_sales_header[\"TotalDue\"].cast(\"float\"))\n",
    "spark.registerDataFrameAsTable(df_sales_header,\"df_sales_header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_due_september = spark.sql(\"\"\"SELECT\n",
    "        SalesOrderID,\n",
    "        OrderDate,\n",
    "        TotalDue\n",
    "    FROM df_sales_header \n",
    "    WHERE (OrderDate BETWEEN '2011-09-01'AND '2011-09-30') AND TotalDue > 1000 \n",
    "    ORDER BY TotalDue DESC\n",
    "\n",
    "    \n",
    "        \"\"\")\n",
    "#df_due_september.write.format(\"csv\").option(\"header\", \"true\").save(\"C:\\\\Users\\\\manoe\\\\OneDrive\\\\Área de Trabalho\\\\\\EngDados\\\\roxpartner\\\\tb_item5.csv\",sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------+\n",
      "|SalesOrderID| OrderDate| TotalDue|\n",
      "+------------+----------+---------+\n",
      "|       44348|2011-09-07|3953.9883|\n",
      "|       44372|2011-09-09|3953.9883|\n",
      "|       44349|2011-09-07|3953.9883|\n",
      "|       44350|2011-09-07|3953.9883|\n",
      "|       44371|2011-09-09|3953.9883|\n",
      "|       44351|2011-09-07|3953.9883|\n",
      "|       44328|2011-09-02|3953.9883|\n",
      "|       44352|2011-09-07|3953.9883|\n",
      "|       44330|2011-09-02|3953.9883|\n",
      "|       44332|2011-09-03|3953.9883|\n",
      "|       44370|2011-09-09|3953.9883|\n",
      "|       44357|2011-09-07|3953.9883|\n",
      "|       44338|2011-09-04|3953.9883|\n",
      "|       44358|2011-09-07|3953.9883|\n",
      "|       44340|2011-09-04|3953.9883|\n",
      "|       44359|2011-09-08|3953.9883|\n",
      "|       44344|2011-09-06|3953.9883|\n",
      "|       44360|2011-09-08|3953.9883|\n",
      "|       44347|2011-09-06|3953.9883|\n",
      "|       44361|2011-09-08|3953.9883|\n",
      "+------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_due_september.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
